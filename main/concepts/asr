1.1 what's difference between repartion and coalesce in spark?
repartition() is used to increase or decrease the RDD/DataFrame partitions whereas the PySpark coalesce() is used to only decrease the number of partitions in an efficient way

1.2 why are spark repartion and coalesce expensive?
because they are very expensive operations as they shuffle the data across many partitions hence try to minimize using these as much as possible.

1.3 what's function of spark coalesce?
it's used only to reduce the number of partitions. This is an optimized or improved version of repartition() where the movement of the data across the partitions is lower using coalesce.

2.1: o que é cluster?
são maquinas que operam com o mesmo objetivo
dividindo o processamento entre todos o computadores
é a determinação de um processo que ocorre com o mesmo objetivo

2.2: arquitetura parecida com um cluster mas não é?
rede de computadores com email, storage não é um cluster

2.3: quais sao os metodos de tolerancia a falha do spark?
replicação, copiam os dados entre os nós do cluster
são divididos em copias espalhados no cluster

2.4: como o spark ajuda no processamento?
particionamento, processando os dados em parelelo em nós separados, dividindo os dados em particões, e essa partições são replicadas em varios nós tendo maior disponibilidade

5: objetivo principal do spark?
processar dados, produzindo informação processada
é uma ferramenta de processamento de dados massivos

6: spark é um projeto ativo?
sim

7: como surgiu o hadoop?
com o google file system(GFS), MapReduce e bigtable

8: hive foi criado por qual empresa?
facebook

9: hive utilizada qual linguagem sobre o hdfs?
sql

-------------------------------------------------------------------------------------------------------
10: o que é o spark?
uma ferramenta de alta performance de processamento de dados, não um storage de dados

11: quais são as principais caracteristicas do spark?
é distribuido em cluster
em memoria, toda operação ocorre em memoria
veloz
facilmente escalavel por conta de ser em cluster
dados em hdfs ou cloud
suporte a particionamento
ferramenta que opera em cluster
replicação e tolerancia a falhas

12: o que é hdfs?
sistema de arquivos distribuido

13: o que é cluster?
é uma rede de computadores/maquinas que operam com o mesmo objetivo

14: como funciona a replicação e tolerancia a falha no spark?
dados são copiados entre os nós do cluster

15: qual o custo que tem de processar dados?
custo computacional

16: qual a origem da linguagem do spark?
scala

--------------------------------------------------------

17: quais os principais componentes do spark?
machine learning (mlib)
sql(spark sql)
processamento em streming
processamento de grafos (graphX)

18: o que o sparksql permite?
ler dados tabulares de varias fontes, usando sintaxe sql

19: spark streming quais suas caracteristicas?
processar dados estruturados
novos registros adicionados ao final da tabela

20: como o spark funciona?
ele constroi grafos aciclicos dirigidos, onde o grafo é uma estrutura de dados compostas por vertices e arestas

21: o que é uma grafo?
o grafo é uma estrutura de dados compostas por vertices e arestas

22: o que é aciclicos dirigidos?
aciclicos: nao tem ciclo (do vertice voce nao consegue voltar pra ele mesmo)
dirigidos: tem uma direção 

23: o que é tungsten?
é o motor de execução do spark, onde o foco dele é a eficiencia da cpu

24: qual é a estrutura do spark?
driver, manager, executer

25: o que o driver faz?
inicializa a sparkSession, solicita recursos computacionais do cluster manager, transforma as operações em DAG's, distribui pelo executers

26: o que o manager faz?
gerencia recursos do cluster

27: quais managers são possiveis de utilizar com o spark?
buit-in standalone, yarn, mesos e kubernetes

28: o que o executer faz?
roda em cada nó do cluster executando tarefas

29: cite 2 elementos do spark?
SparkSession: seção
aplications: Programa

30: dentro de transformaçoes e ações, qual o comportamento do dataframe?
é uma estrutura de dados imutável

31: quando uma tranformação ocorre no dataframe o que ocorre?
ele gera um novo dataframe

32: qual a vantagem do dado ser imutável?
traz tolerancia a falhas

33: quando ocorre o processamento de transformação de fato no spark?
quando ocorre uma ação

34: quando processamos dados no spark quais os 2 tipos de operações que fazemos?
transformações e ações

35: o que lazy evaluation no spark?
onde voce pode fazer várias transformações no seu dado, o spark irá criar um plano de execução e só irá de fato executar quando ocorrer uma ação

36: quais os 2 tipos que podem ser uma transformação?
narrow e wide

37: o que é narrow?
quando a transformação que ocorre no dado está na mesma partição

38: o que é wide?
quando a transformação ocorre no dado que está em mais de uma partição

39: como os dados do spark são processados (componentes)?
job: tarefa que tem ser executada
stage: Divisão do job
task: menor unidade de trabalho. Uma por núcleo e por partição
--------------------------------------------------------

39: o que é SparkContext?
é uma conexão transparente com o cluster

40: o que é SparkSession?
acesso ao SparkContext

----------------------------------------

41: como eram armazenados os dados antigamente?
em algum tipo de sistema gerenciador de dado em formato proprietario

42: quais são os principais formatos de arquivos abertos para big data?
parquet, avro, apache orc

43: quais são caracteristicas dos armazem de dados em big data atualmente?
armazenamento de dados em formatos desacoplados de ferramentas e abertos
formatos binários, compactados
suportam schema
podem ser particionados entre discos (redundancia e parelelismo)

44: o que é formato de dado parquet?
sistema de arquivos colunar (perfomance maior para read)

45: o que é formato de dado arvro?
sistema de arquivos em linha (tem performance melhor para write)

46: o que é formato de dado orc?
sistema de arquivos em colunar, padrão do hive (perfomance maior para read)

47: qual a melhor perfomance para escrita do dado linha ou coluna?
linha

48: como os banco de dados relacionais são otimizados em perfomance?
escrita em linha

49: em geral qual é melhor na criação e compressão, parquet ou orc?
orc

50: em geral qual é melhor na performance, parquet ou orc?
parquet

51: qual é formato de arquivos padrão do spark?
parquet







