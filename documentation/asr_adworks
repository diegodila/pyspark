#1
from pyspark.sql.functions import lit, concat, current_timestamp, current_date
spark.range(1).select(current_date().alias("doifjdoijf"), concat("current_date",lit("limao")).alias('limao'),current_timestamp().alias("difujdiuf"))

#2
from pyspark.sql.functions import current_timestamp
spark.range(1).select(current_timestamp().alias("current_timestamp"))

#3
from pyspark.sql.functions import col, current_date
spark.read.option('header',True).csv('./work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .where(col("ModifiedDate").astype('date').between('2014',current_date())).count()
    
#4
from pyspark.sql.functions import month, year
spark.range(1).select(month(current_date()).alias("month"),year(current_date()).alias("year"))

#5
from pyspark.sql.functions import month, year
spark.range(1).select(month(current_date()).alias("month"))

#6
from pyspark.sql.functions import col
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Sales/SalesOrderDetail.csv') \
    .groupBy("SpecialOfferID").agg(sum(col("UnitPrice").cast('int')).alias('sum_UnitPrice'))
    
#7
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Sales/SalesOrderDetail.csv') \
    .groupBy('SpecialOfferID').count()
    
#8
from pyspark.sql.functions import count
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Sales/SalesOrderDetail.csv') \
    .groupBy('ProductID').agg(count("ProductID").alias("ProductID_count")).orderBy(col("ProductID"))
    
#9
from pyspark.sql.functions import count
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .groupBy('FirstName').agg(count('FirstName').alias('FirstName_count')).orderBy(col('FirstName'))
    
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .select(col("Firstname")).distinct()
    
#10
from pyspark.sql.functions import avg
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Production/Product.csv') \
    .where(col("Color") == "Silver") \
        .groupBy('Color').agg(avg("ListPrice").alias('ListPrice_avg'))
        
#11
from pyspark.sql.functions import when, trim
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Production/Product.csv') \
    .select("ProductNumber", 'Name','ProductLine',when(trim(col("ProductLine")) == 'R','Road') \
        .when(trim(col("ProductLine")) == 'M','Montain') \
            .when(trim(col("ProductLine")) == 'T','Touring') \
                .when(trim(col("ProductLine")) == 'S', 'Other sale items') \
                    .otherwise('Not for Sale').alias("Category")).orderBy("ProductNumber")

#12
from pyspark.sql.functions import when, col
spark.read.format('csv').option('header',True).load('work/pyspark/documentation/data/AdventureWorks/Production/Product.csv') \
    .select(col("ProductNumber"), col("Name"), when(col("ListPrice") == 0, "Mfg item - not for resale") \
        .when(col("ListPrice") < 50, "Under 50") \
            .when(col("ListPrice") < 250, "Under 250") \
                .when(col("ListPrice") < 1000, "Under 1000") \
                    .otherwise('Over 1000').alias("pricerange"), col("ListPrice")).orderBy("ProductNumber")

#13
from pyspark.sql.functions import col, coalesce
spark.read.format('csv').option('header',True).load('work/pyspark/documentation/data/AdventureWorks/Production/Product.csv') \
    .select(col('Class'), col('Color'), col('ProductNumber'), coalesce(col('Class'),col('Color'), col('ProductNumber')))

#14
from pyspark.sql.functions import coalesce, col
spark.read.format('csv').option('header',True).load('work/pyspark/documentation/data/AdventureWorks/Production/Product.csv') \
    .select(col("Color"), col('ProductNumber'), coalesce(col("Color"), col('ProductNumber')).alias('coalesce'))

#15
from pyspark.sql.functions import col
spark.read.format('csv').option('header',True).load('work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .where((col('Suffix').isNull()))

#16
from pyspark.sql.functions import col, coalesce,lit
spark.read.format('csv').option('header',True).load('work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .fillna('goiaba*',['Suffix'])
    
spark.read.format('csv').option('header',True).load('work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .select(coalesce(col('Suffix'), lit('Goiaba*')).alias('Suffix_n'),"*").drop(col('Suffix'))

#17
from pyspark.sql.funtions import coalesce, col
spark.read.format('csv').option('header',True).load('work/pyspark/documentation/data/AdventureWorks/Person/Address.csv') \
    .select(coalesce(col('AddressLine2'), col('AddressLine1')).alias('new_column'),"*")

#18
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .groupBy('FirstName').count().filter(col('count') > 10)

#19
from pyspark.sql.functions import count
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .groupBy(col('LastName')).agg(count(col('LastName')).alias('count_last_name')).filter(col('count_last_name') > 2).orderBy('LastName')

#20
from pyspark.sql.functions import col
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .where(col('FirstName').isin(['Greg', 'Jeff', 'Sheena'])) \
        .groupBy('FirstName').count().filter(col('count') > 10)

#21
from pyspark.sql.functions import col, sum
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Sales/SalesOrderDetail.csv') \
    .groupBy('ProductId').agg(sum(col('LineTotal')).alias('sum_unit_price')).filter(col('sum_unit_price').between(162000,500000))