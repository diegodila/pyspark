#1
from pyspark.sql.functions import lit, concat, current_timestamp, current_date
spark.range(1).select(current_date().alias("doifjdoijf"), concat("current_date",lit("limao")).alias('limao'),current_timestamp().alias("difujdiuf"))

#2
from pyspark.sql.functions import current_timestamp
spark.range(1).select(current_timestamp().alias("current_timestamp"))

#3
from pyspark.sql.functions import col, current_date
spark.read.option('header',True).csv('./work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .where(col("ModifiedDate").astype('date').between('2014',current_date())).count()
    
#4
from pyspark.sql.functions import month, year
spark.range(1).select(month(current_date()).alias("month"),year(current_date()).alias("year"))

#5
from pyspark.sql.functions import month, year
spark.range(1).select(month(current_date()).alias("month"))

#6
from pyspark.sql.functions import col
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Sales/SalesOrderDetail.csv') \
    .groupBy("SpecialOfferID").agg(sum(col("UnitPrice").cast('int')).alias('sum_UnitPrice'))
    
#7
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Sales/SalesOrderDetail.csv') \
    .groupBy('SpecialOfferID').count()
    
#8
from pyspark.sql.functions import count
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Sales/SalesOrderDetail.csv') \
    .groupBy('ProductID').agg(count("ProductID").alias("ProductID_count")).orderBy(col("ProductID"))
    
#9
from pyspark.sql.functions import count
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Person/Person.csv') \
    .groupBy('FirstName').agg(count('FirstName').alias('FirstName_count')).orderBy(col('FirstName'))
    
    
#10
from pyspark.sql.functions import avg
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Production/Product.csv') \
    .where(col("Color") == "Silver") \
        .groupBy('Color').agg(avg("ListPrice").alias('ListPrice_avg'))
        
#11
from pyspark.sql.functions import when, trim
spark.read.option('header',True).csv('work/pyspark/documentation/data/AdventureWorks/Production/Product.csv') \
    .select("ProductNumber", 'Name','ProductLine',when(trim(col("ProductLine")) == 'R','Road') \
        .when(trim(col("ProductLine")) == 'M','Montain') \
            .when(trim(col("ProductLine")) == 'T','Touring') \
                .when(trim(col("ProductLine")) == 'S', 'Other sale items') \
                    .otherwise('Not for Sale').alias("Category")).orderBy("ProductNumber")