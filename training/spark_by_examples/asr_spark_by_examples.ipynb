{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.maxNumRows',20)\n",
    "spark.conf.set('sparl.sql.repl.eagerEval.truncate',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime, date\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1,b=2.,c='string1',d=date(2000,1,1),e=datetime(2000,1,1,12,0)),\n",
    "    Row(a=2,b=3.,c='string2',d=date(2000,2,1),e=datetime(2000,1,2,13,0)),\n",
    "    Row(a=3,b=5.,c='string3',d=date(2000,3,1),e=datetime(2000,1,3,14,0)),\n",
    "    Row(a=4,b=7.,c='string4',d=date(2000,4,1),e=datetime(2000,1,4,15,0))\n",
    "])\n",
    "\n",
    "data_dict = [{'a':'6','b':'5','c':'4'}]\n",
    "df_dict = spark.createDataFrame(data_dict)\n",
    "\n",
    "data_tup = [(1,2,3)]\n",
    "columns_tup = ('a','b','c')\n",
    "df_tup = spark.createDataFrame(data=data_tup,schema=columns_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "df_explicit_schema  = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df_explicit_schema.show()\n",
    "df_explicit_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "df_explicit_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#6\n",
    "# Display 2 rows & column values 25 characters\n",
    "df.show(2,truncate=25) \n",
    "\n",
    "#+-----+-------------------------+\n",
    "#|Seqno|                    Quote|\n",
    "#+-----+-------------------------+\n",
    "#|    1|Be the change that you...|\n",
    "#|    2|Everyone thinks of cha...|\n",
    "#+-----+-------------------------+\n",
    "#only showing top 2 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "# Display DataFrame rows & columns vertically\n",
    "df.show(n=3,truncate=25,vertical=True)\n",
    "\n",
    "#-RECORD 0--------------------------\n",
    "# Seqno | 1                         \n",
    "# Quote | Be the change that you... \n",
    "#-RECORD 1--------------------------\n",
    "# Seqno | 2                         \n",
    "# Quote | Everyone thinks of cha... \n",
    "#-RECORD 2--------------------------\n",
    "# Seqno | 3                         \n",
    "# Quote | The purpose of our liv... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "from pyspark.sql.types import StructType,StructField, IntegerType, StringType\n",
    "\n",
    "data = [(1,23,4,5,5,3,533),(1,3,4,45,5,3,333),(1,3,4,5,5,3,333),(14,3,4,5,5,3,533),(1,3,4,5,5,33,3)]\n",
    "\n",
    "columns = StructType([StructField(\"id\",IntegerType(),True),\n",
    "                     StructField(\"number\",IntegerType(),True),\n",
    "                     StructField('number3',IntegerType(),True),\n",
    "                     StructField('number4',IntegerType(),True),\n",
    "                     StructField('number5',IntegerType(),True),\n",
    "                     StructField('number6',IntegerType(),True),\n",
    "                     StructField('number7',IntegerType(),True)])\n",
    "\n",
    "df = spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "from pyspark.sql import Row\n",
    "row=Row(\"James\",40)\n",
    "print(row[0] +\",\"+str(row[1]))\n",
    "\n",
    "\n",
    "row=Row(name=\"Alice\", age=11)\n",
    "print(row.name)\n",
    "\n",
    "from pyspark.sql.types import Row, StringType, StructType, StructField\n",
    "data = Row('1','2')\n",
    "schema = StructType([StructField(\"a\",StringType(),True),StructField('b',StringType(),True)])\n",
    "df = spark.createDataFrame([data],schema)\n",
    "\n",
    "data = Row(a=1, c=2)\n",
    "spark.createDataFrame([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "\n",
    "Person = Row(\"name\", \"age\")\n",
    "p1=Person(\"James\", 40)\n",
    "p2=Person(\"Alice\", 35)\n",
    "print(p1.name +\",\"+p2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "from pyspark.sql.functions import rank\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "from pyspark.sql.functions import dense_rank\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14\n",
    "from pyspark.sql.functions import percent_rank\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15\n",
    "from pyspark.sql.functions import ntile\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16\n",
    "from pyspark.sql.functions import cume_dist\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "from pyspark.sql.functions import lag \n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18\n",
    "from pyspark.sql.functions import lead\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()\n",
    "  \n",
    "df.groupBy('department') \\\n",
    "    .agg(avg('salary'), \\\n",
    "        sum('salary'), \\\n",
    "        min('salary'), \\\n",
    "        max('salary')) \\\n",
    "    .orderBy('department').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20\n",
    "from pyspark.sql.functions import max\n",
    "dado = [('we', 20),('sopd', 30),('siod',54)]\n",
    "df_first = spark.createDataFrame(dado,schema='a string, b int')\n",
    "df_first.select(max('b')).first()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
