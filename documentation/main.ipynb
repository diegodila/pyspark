{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T21:08:11.718952331Z",
     "start_time": "2023-06-20T21:08:08.056548674Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Notebook de testes').getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled',True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.maxNumRows',25)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.truncate',30)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>StateProvinceID</th><th>count(StateProvinceID)</th></tr>\n",
       "<tr><td>7</td><td>1579</td></tr>\n",
       "<tr><td>15</td><td>31</td></tr>\n",
       "<tr><td>54</td><td>16</td></tr>\n",
       "<tr><td>11</td><td>9</td></tr>\n",
       "<tr><td>42</td><td>8</td></tr>\n",
       "<tr><td>73</td><td>46</td></tr>\n",
       "<tr><td>64</td><td>795</td></tr>\n",
       "<tr><td>3</td><td>7</td></tr>\n",
       "<tr><td>30</td><td>8</td></tr>\n",
       "<tr><td>59</td><td>1</td></tr>\n",
       "<tr><td>8</td><td>231</td></tr>\n",
       "<tr><td>85</td><td>32</td></tr>\n",
       "<tr><td>35</td><td>18</td></tr>\n",
       "<tr><td>52</td><td>8</td></tr>\n",
       "<tr><td>71</td><td>106</td></tr>\n",
       "<tr><td>179</td><td>288</td></tr>\n",
       "<tr><td>31</td><td>2</td></tr>\n",
       "<tr><td>163</td><td>62</td></tr>\n",
       "<tr><td>70</td><td>453</td></tr>\n",
       "<tr><td>27</td><td>6</td></tr>\n",
       "<tr><td>75</td><td>6</td></tr>\n",
       "<tr><td>166</td><td>22</td></tr>\n",
       "<tr><td>17</td><td>17</td></tr>\n",
       "<tr><td>131</td><td>62</td></tr>\n",
       "<tr><td>46</td><td>5</td></tr>\n",
       "</table>\n",
       "only showing top 25 rows\n"
      ],
      "text/plain": [
       "+---------------+----------------------+\n",
       "|StateProvinceID|count(StateProvinceID)|\n",
       "+---------------+----------------------+\n",
       "|              7|                  1579|\n",
       "|             15|                    31|\n",
       "|             54|                    16|\n",
       "|             11|                     9|\n",
       "|             42|                     8|\n",
       "|             73|                    46|\n",
       "|             64|                   795|\n",
       "|              3|                     7|\n",
       "|             30|                     8|\n",
       "|             59|                     1|\n",
       "|              8|                   231|\n",
       "|             85|                    32|\n",
       "|             35|                    18|\n",
       "|             52|                     8|\n",
       "|             71|                   106|\n",
       "|            179|                   288|\n",
       "|             31|                     2|\n",
       "|            163|                    62|\n",
       "|             70|                   453|\n",
       "|             27|                     6|\n",
       "|             75|                     6|\n",
       "|            166|                    22|\n",
       "|             17|                    17|\n",
       "|            131|                    62|\n",
       "|             46|                     5|\n",
       "+---------------+----------------------+\n",
       "only showing top 25 rows"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "spark.read.format('csv').option('header',True).load('work/pyspark/documentation/data/AdventureWorks/Person/Address.csv') \\\n",
    "    .groupBy(col('StateProvinceID')).agg(count(col('StateProvinceID'))).join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('csv').option('header',True).load('work/pyspark/documentation/data/AdventureWorks/Person/BusinessEntity.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##  23. Identificar as provincias(stateProvinceID) com o maior numero de cadastros no nosso sistema, é preciso encontrar provincias que estão registradas no banco de dados mais que 1000 vezes (trazer os nomes das provincias tambem)\n",
    "\n",
    "##  24. Selecione o TerritoryName, BusinessEntityID, SalesYTD, desloque o salesYTD 1 linha, na janela do territoryName (vSalesPerson)\n",
    "\n",
    "##  25. Contar os nomes de registros duplicados da tabela Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
