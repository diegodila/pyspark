1.1 what's difference between repartion and coalesce in spark?
repartition() is used to increase or decrease the RDD/DataFrame partitions whereas the PySpark coalesce() is used to only decrease the number of partitions in an efficient way

1.2 why are spark repartion and coalesce expensive?
because they are very expensive operations as they shuffle the data across many partitions hence try to minimize using these as much as possible.

1.3 what's function of spark coalesce?
it's used only to reduce the number of partitions. This is an optimized or improved version of repartition() where the movement of the data across the partitions is lower using coalesce.

2.1: o que é cluster?
são maquinas que operam com o mesmo objetivo
dividindo o processamento entre todos o computadores
é a determinação de um processo que ocorre com o mesmo objetivo

2.2: arquitetura parecida com um cluster mas não é?
rede de computadores com email, storage não é um cluster

2.3: quais sao os metodos de tolerancia a falha do spark?
replicação, copiam os dados entre os nós do cluster
são divididos em copias espalhados no cluster

2.4: como o spark trabalha no processamento?
com particionamento, processando os dados em parelelo em nós separados, dividindo os dados em particões, e essa partições são replicadas em varios nós tendo maior disponibilidade

2.5: objetivo principal do spark?
processar dados, produzindo informação processada
é uma ferramenta de processamento de dados massivos

2.6: spark é um projeto ativo?
sim

2.7: como surgiu o hadoop?
com o google file system(GFS), MapReduce e bigtable

2.8: hive foi criado por qual empresa?
facebook

2.9: hive utilizada qual linguagem sobre o hdfs?
sql

2.10: o que é o spark?
uma ferramenta de alta performance de processamento de dados, não um storage de dados

2.11: 5 principais caracteristicas do spark?
é distribuido em cluster
em memoria, toda operação ocorre em memoria
veloz
facilmente escalavel por conta de ser em cluster - alta disponibilidade
dados em hdfs ou cloud
suporte a particionamento
ferramenta que opera em cluster
replicação e tolerancia a falhas

2.12: o que é hdfs?
sistema de arquivos distribuido

2.13: o que é cluster?
é uma rede de computadores/maquinas que operam com o mesmo objetivo

2.14: como funciona a replicação e tolerancia a falha no spark?
dados são copiados entre os nós do cluster

2.15: qual o custo que tem de processar dados?
custo computacional

2.16: qual linguagem utilizada na origem do spark?
scala

2.17: 4 principais componentes do spark?
machine learning (mlib)
sql(spark sql)
processamento em streming
processamento de grafos (graphX)

2.18: o que o sparksql permite?
ler dados tabulares de varias fontes, usando sintaxe sql

2.19: spark streaming quais suas caracteristicas?
processar dados estruturados
novos registros adicionados ao final da tabela

2.20: como o spark funciona por debaixo dos panos?
ele constroi grafos aciclicos dirigidos, onde o grafo é uma estrutura de dados compostas por vertices e arestas

2.21: o que é uma grafo?
o grafo é uma estrutura de dados compostas por vertices e arestas

2.22: o que é aciclicos dirigidos?
aciclicos: nao tem ciclo (do vertice voce nao consegue voltar pra ele mesmo)
dirigidos: tem uma direção 

2.23: o que é tungsten?
é o motor de execução do spark, onde o foco dele é a eficiencia da cpu

2.24: qual é a estrutura do spark?
driver, manager, executer

2.25: o que o driver faz?
inicializa a sparkSession, solicita recursos computacionais do cluster manager, transforma as operações em DAG's, distribui pelo executers

2.26: o que o manager faz?
gerencia recursos do cluster

2.27: quais managers são possiveis de utilizar com o spark?
buit-in standalone, yarn, mesos e kubernetes

2.28: o que o executer faz?
roda em cada nó do cluster executando tarefas

2.29: cite 2 elementos do spark?
SparkSession: seção
aplications: Programa

2.30: dentro de transformaçoes e ações, qual o comportamento do dataframe?
é uma estrutura de dados imutável

2.31: quando uma tranformação ocorre no dataframe o que ocorre?
ele gera um novo dataframe

2.32: qual a vantagem do dado ser imutável?
traz tolerancia a falhas

2.33: quando ocorre o processamento de transformação de fato no spark?
quando ocorre uma ação

2.34: quando processamos dados no spark quais os 2 tipos de operações que fazemos?
transformações e ações

2.35: o que lazy evaluation no spark?
onde voce pode fazer várias transformações no seu dado, o spark irá criar um plano de execução e só irá de fato executar quando ocorrer uma ação

2.36: quais os 2 tipos que podem ser uma transformação?
narrow e wide

2.37: o que é narrow?
quando a transformação que ocorre no dado está na mesma partição

2.38: o que é wide?
quando a transformação ocorre no dado que está em mais de uma partição

2.39: como os dados do spark são processados (componentes)?
job: tarefa que tem ser executada
stage: Divisão do job
task: menor unidade de trabalho. Uma por núcleo e por partição
--------------------------------------------------------

2.39: o que é SparkContext?
é uma conexão transparente com o cluster

2.40: o que é SparkSession?
acesso ao SparkContext

----------------------------------------

2.41: como eram armazenados os dados antigamente?
em algum tipo de sistema gerenciador de dado em formato proprietario

2.42: quais são os principais formatos de arquivos abertos para big data?
parquet, avro, apache orc

2.43: quais são caracteristicas dos armazem de dados em big data atualmente?
armazenamento de dados em formatos desacoplados de ferramentas e abertos
formatos binários, compactados
suportam schema
podem ser particionados entre discos (redundancia e parelelismo)

2.44: o que é formato de dado parquet?
sistema de arquivos colunar (perfomance maior para read)

2.45: o que é formato de dado avro?
sistema de arquivos em linha (tem performance melhor para write)

2.46: o que é formato de dado orc?
sistema de arquivos em colunar, padrão do hive (perfomance maior para read)

2.47: qual a melhor perfomance para escrita do dado linha ou coluna?
linha

2.48: como os banco de dados relacionais são otimizados em perfomance?
escrita em linha

2.49: em geral qual é melhor na criação e compressão, parquet ou orc?
orc

2.50: em geral qual é melhor na performance, parquet ou orc?
parquet

2.51: qual é formato de arquivos padrão do spark?
parquet







